{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.014512,
     "end_time": "2021-03-01T05:13:11.679869",
     "exception": false,
     "start_time": "2021-03-01T05:13:11.665357",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "warnings.simplefilter('ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-01T05:13:11.709116Z",
     "iopub.status.busy": "2021-03-01T05:13:11.708530Z",
     "iopub.status.idle": "2021-03-01T05:13:13.700637Z",
     "shell.execute_reply": "2021-03-01T05:13:13.701104Z"
    },
    "papermill": {
     "duration": 2.008616,
     "end_time": "2021-03-01T05:13:13.701482",
     "exception": false,
     "start_time": "2021-03-01T05:13:11.692866",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.tag import pos_tag\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "from pprint import pprint\n",
    "\n",
    "# Gensim\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim  # don't skip this\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.01275,
     "end_time": "2021-03-01T05:13:13.727818",
     "exception": false,
     "start_time": "2021-03-01T05:13:13.715068",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 1. Read the .csv file using Pandas. Take a look at the top few records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-01T05:13:13.757161Z",
     "iopub.status.busy": "2021-03-01T05:13:13.756612Z",
     "iopub.status.idle": "2021-03-01T05:13:13.865010Z",
     "shell.execute_reply": "2021-03-01T05:13:13.865889Z"
    },
    "papermill": {
     "duration": 0.125277,
     "end_time": "2021-03-01T05:13:13.866114",
     "exception": false,
     "start_time": "2021-03-01T05:13:13.740837",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Read the .csv file using Pandas. Take a look at the top few records.\n",
    "ReviewData = pd.read_csv('K8 Reviews v0.2.csv')\n",
    "ReviewData.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.013804,
     "end_time": "2021-03-01T05:13:13.894589",
     "exception": false,
     "start_time": "2021-03-01T05:13:13.880785",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 2. Normalize casings for the review text and extract the text into a list for easier manipulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-01T05:13:13.928325Z",
     "iopub.status.busy": "2021-03-01T05:13:13.927609Z",
     "iopub.status.idle": "2021-03-01T05:13:13.931160Z",
     "shell.execute_reply": "2021-03-01T05:13:13.930676Z"
    },
    "papermill": {
     "duration": 0.02246,
     "end_time": "2021-03-01T05:13:13.931287",
     "exception": false,
     "start_time": "2021-03-01T05:13:13.908827",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def Normalize(reviews):\n",
    "    NormalizeReviews = []\n",
    "    for review in reviews:\n",
    "        NormalizeReviews.append(review.lower())\n",
    "    return NormalizeReviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-01T05:13:13.962663Z",
     "iopub.status.busy": "2021-03-01T05:13:13.962004Z",
     "iopub.status.idle": "2021-03-01T05:13:13.976952Z",
     "shell.execute_reply": "2021-03-01T05:13:13.976387Z"
    },
    "papermill": {
     "duration": 0.031562,
     "end_time": "2021-03-01T05:13:13.977085",
     "exception": false,
     "start_time": "2021-03-01T05:13:13.945523",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Normalize casings for the review text and extract the text into a list for easier manipulation.\n",
    "NormalizeReviewText = Normalize(ReviewData['review'].values)\n",
    "NormalizeReviewText"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.014531,
     "end_time": "2021-03-01T05:13:14.007785",
     "exception": false,
     "start_time": "2021-03-01T05:13:13.993254",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 4. Perform parts-of-speech tagging on each sentence using the NLTK POS tagger.\n",
    "\n",
    "# 5. For the topic model, we should  want to include only nouns.\n",
    "\n",
    "    Find out all the POS tags that correspond to nouns.\n",
    "\n",
    "    Limit the data to only terms with these tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-01T05:13:14.043826Z",
     "iopub.status.busy": "2021-03-01T05:13:14.043146Z",
     "iopub.status.idle": "2021-03-01T05:13:14.045988Z",
     "shell.execute_reply": "2021-03-01T05:13:14.046518Z"
    },
    "papermill": {
     "duration": 0.023898,
     "end_time": "2021-03-01T05:13:14.046677",
     "exception": false,
     "start_time": "2021-03-01T05:13:14.022779",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def Tokenize_POS(reviews):\n",
    "    TokenizeReviews = []\n",
    "    for review in reviews:\n",
    "        #review = nltk.word_tokenize(review)\n",
    "        #TokenizeReviews.append(nltk.pos_tag(review))  \n",
    "        for word,pos in nltk.pos_tag(nltk.word_tokenize(review)):\n",
    "            if (pos == 'NN' or pos == 'NNP' or pos == 'NNS' or pos == 'NNPS'):\n",
    "                #review = lemmatizer.lemmatize(word)\n",
    "                #print (word)\n",
    "                TokenizeReviews.append(review)    \n",
    "    return TokenizeReviews    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-01T05:13:14.092629Z",
     "iopub.status.busy": "2021-03-01T05:13:14.087886Z",
     "iopub.status.idle": "2021-03-01T05:13:14.096607Z",
     "shell.execute_reply": "2021-03-01T05:13:14.097018Z"
    },
    "papermill": {
     "duration": 0.035292,
     "end_time": "2021-03-01T05:13:14.097179",
     "exception": false,
     "start_time": "2021-03-01T05:13:14.061887",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Tokenize the reviews using NLTKs word_tokenize function.\n",
    "#Perform parts-of-speech tagging on each sentence using the NLTK POS tagger.\n",
    "TokenizeReviews = Tokenize_POS(NormalizeReviewText)\n",
    "TokenizeReviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.015382,
     "end_time": "2021-03-01T05:13:14.128327",
     "exception": false,
     "start_time": "2021-03-01T05:13:14.112945",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 6. Lemmatize. \n",
    "\n",
    "    Different forms of the terms need to be treated as one.\n",
    "    No need to provide POS tag to lemmatizer for now.\n",
    "\n",
    "# 7. Remove stopwords and punctuation (if there are any). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-01T05:13:14.162771Z",
     "iopub.status.busy": "2021-03-01T05:13:14.162128Z",
     "iopub.status.idle": "2021-03-01T05:13:14.168488Z",
     "shell.execute_reply": "2021-03-01T05:13:14.168904Z"
    },
    "papermill": {
     "duration": 0.024937,
     "end_time": "2021-03-01T05:13:14.169069",
     "exception": false,
     "start_time": "2021-03-01T05:13:14.144132",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# function to remove Stopwords\n",
    "def Remove_Stopwords(word_list, lang='english'):\n",
    "    \"\"\"Function removes english stopwords\n",
    "    Args:\n",
    "        word_list  : list of words\n",
    "    Return:\n",
    "        The return value. List of words\n",
    "    \"\"\"\n",
    "    content = []\n",
    "    stopwords_list = stopwords.words(lang)\n",
    "    #print(type(word_list))\n",
    "    #for word in word_list:\n",
    "    #    print(word)\n",
    "    #    if word.lower() not in stopwords_list:\n",
    "    #        content.append(word)\n",
    "    content = [w for w in word_list if w.lower() not in stopwords_list]\n",
    "    #print(content)\n",
    "    return content\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-01T05:13:14.206718Z",
     "iopub.status.busy": "2021-03-01T05:13:14.206055Z",
     "iopub.status.idle": "2021-03-01T05:13:14.209420Z",
     "shell.execute_reply": "2021-03-01T05:13:14.209851Z"
    },
    "papermill": {
     "duration": 0.02438,
     "end_time": "2021-03-01T05:13:14.210012",
     "exception": false,
     "start_time": "2021-03-01T05:13:14.185632",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# function to remove punctuation\n",
    "def Simplify_Punctuation(text):\n",
    "    \"\"\"\n",
    "    This function simplifies doubled or more complex punctuation. The exception is '...'.\n",
    "    \"\"\"\n",
    "    corrected = str(text)\n",
    "    corrected = re.sub(r'([!?,;])\\1+', r'\\1', corrected)\n",
    "    corrected = re.sub(r'\\.{2,}', r'...', corrected)\n",
    "    return corrected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-01T05:13:14.248238Z",
     "iopub.status.busy": "2021-03-01T05:13:14.247321Z",
     "iopub.status.idle": "2021-03-01T05:13:14.251943Z",
     "shell.execute_reply": "2021-03-01T05:13:14.252633Z"
    },
    "papermill": {
     "duration": 0.026492,
     "end_time": "2021-03-01T05:13:14.252855",
     "exception": false,
     "start_time": "2021-03-01T05:13:14.226363",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# function to lemmatize using WordNetLemmatizer\n",
    "def Lemmatize_WordNet(words_list):\n",
    "    wnl = WordNetLemmatizer()\n",
    "    encoded_list = []\n",
    "    for word in words_list:\n",
    "        encoded_list.append(wnl.lemmatize(word, pos=\"v\"))#.encode(\"utf8\"))\n",
    "    #print(encoded_list)\n",
    "    return encoded_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-01T05:13:14.292805Z",
     "iopub.status.busy": "2021-03-01T05:13:14.292037Z",
     "iopub.status.idle": "2021-03-01T05:13:14.294889Z",
     "shell.execute_reply": "2021-03-01T05:13:14.295529Z"
    },
    "papermill": {
     "duration": 0.02543,
     "end_time": "2021-03-01T05:13:14.295736",
     "exception": false,
     "start_time": "2021-03-01T05:13:14.270306",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def tokenize(txt):\n",
    "    \"\"\"Function computes Tokenizes into sentences, strips punctuation/abbr, \n",
    "       converts to lowercase and tokenizes words\n",
    "    Args:\n",
    "        txt  : text documents\n",
    "    Return:\n",
    "        The return value. Tokenized words\n",
    "    \"\"\"\n",
    "    return [word_tokenize(\" \".join(re.findall(r'\\w+', t,flags = re.UNICODE )).lower()) \n",
    "                for t in sent_tokenize(txt.replace(\"'\", \"\"))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-01T05:13:14.337911Z",
     "iopub.status.busy": "2021-03-01T05:13:14.337245Z",
     "iopub.status.idle": "2021-03-01T05:13:14.339973Z",
     "shell.execute_reply": "2021-03-01T05:13:14.340458Z"
    },
    "papermill": {
     "duration": 0.026366,
     "end_time": "2021-03-01T05:13:14.340619",
     "exception": false,
     "start_time": "2021-03-01T05:13:14.314253",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def Apply_Stopwords_punctuation_lemmatize(reviews):\n",
    "    PreprocessReviews = []\n",
    "    for review in reviews:\n",
    "        lemmetized = []\n",
    "        review = Simplify_Punctuation(review)  # Remove Punctuation        \n",
    "        sentences = tokenize(review)\n",
    "        for sentence in sentences:\n",
    "            words = Remove_Stopwords(sentence)         # Remove Stopwords\n",
    "            words = Lemmatize_WordNet(words)           # lemmatize \n",
    "            # lets's skip short sentences with less than 3 words\n",
    "            if len(words) < 3:\n",
    "                continue\n",
    "            lemmetized.append(\" \".join(words))\n",
    "        PreprocessReviews.append(\" \".join(lemmetized))\n",
    "    return PreprocessReviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.017196,
     "end_time": "2021-03-01T05:13:14.375574",
     "exception": false,
     "start_time": "2021-03-01T05:13:14.358378",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Lemmatize. \n",
    "Different forms of the terms need to be treated as one.\n",
    "No need to provide POS tag to lemmatizer for now.\n",
    "Remove stopwords and punctuation (if there are any). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-01T05:13:14.423681Z",
     "iopub.status.busy": "2021-03-01T05:13:14.422958Z",
     "iopub.status.idle": "2021-03-01T05:13:14.428774Z",
     "shell.execute_reply": "2021-03-01T05:13:14.428261Z"
    },
    "papermill": {
     "duration": 0.035976,
     "end_time": "2021-03-01T05:13:14.428914",
     "exception": false,
     "start_time": "2021-03-01T05:13:14.392938",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "PreProcessReviews = Apply_Stopwords_punctuation_lemmatize(TokenizeReviews)\n",
    "PreProcessReviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.017996,
     "end_time": "2021-03-01T05:13:14.465204",
     "exception": false,
     "start_time": "2021-03-01T05:13:14.447208",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 8. Create a topic model using LDA on the cleaned-up data with 12 topics.\n",
    "\n",
    "    Print out the top terms for each topic.\n",
    "    What is the coherence of the model with the c_v metric?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-01T05:13:14.512259Z",
     "iopub.status.busy": "2021-03-01T05:13:14.507453Z",
     "iopub.status.idle": "2021-03-01T05:13:14.523659Z",
     "shell.execute_reply": "2021-03-01T05:13:14.523169Z"
    },
    "papermill": {
     "duration": 0.039947,
     "end_time": "2021-03-01T05:13:14.523809",
     "exception": false,
     "start_time": "2021-03-01T05:13:14.483862",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "TokenizeReviews = []\n",
    "for review in PreProcessReviews:\n",
    "    TokenizeReviews.append(nltk.word_tokenize(review)) \n",
    "#TokenizeReviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-01T05:13:14.574922Z",
     "iopub.status.busy": "2021-03-01T05:13:14.567532Z",
     "iopub.status.idle": "2021-03-01T05:13:14.585099Z",
     "shell.execute_reply": "2021-03-01T05:13:14.585828Z"
    },
    "papermill": {
     "duration": 0.043228,
     "end_time": "2021-03-01T05:13:14.586011",
     "exception": false,
     "start_time": "2021-03-01T05:13:14.542783",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create Dictionary\n",
    "\n",
    "id2word = corpora.Dictionary(TokenizeReviews)\n",
    "\n",
    "# Create Corpus\n",
    "texts = TokenizeReviews\n",
    "\n",
    "# Term Document Frequency\n",
    "corpus = [id2word.doc2bow(text) for text in texts]\n",
    "\n",
    "# View\n",
    "print(corpus[:1])\n",
    "print(id2word[0])\n",
    "\n",
    "[[(id2word[id], freq) for id, freq in cp] for cp in corpus[:1]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-01T05:13:14.638226Z",
     "iopub.status.busy": "2021-03-01T05:13:14.637584Z",
     "iopub.status.idle": "2021-03-01T05:13:14.647295Z",
     "shell.execute_reply": "2021-03-01T05:13:14.647779Z"
    },
    "papermill": {
     "duration": 0.042036,
     "end_time": "2021-03-01T05:13:14.647943",
     "exception": false,
     "start_time": "2021-03-01T05:13:14.605907",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Build LDA model\n",
    "lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
    "                                           id2word=id2word,\n",
    "                                           num_topics=12, \n",
    "                                           random_state=100,\n",
    "                                           update_every=1,\n",
    "                                           chunksize=100,\n",
    "                                           passes=10,\n",
    "                                           alpha='auto',\n",
    "                                           per_word_topics=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-01T05:13:14.700984Z",
     "iopub.status.busy": "2021-03-01T05:13:14.700255Z",
     "iopub.status.idle": "2021-03-01T05:13:14.705332Z",
     "shell.execute_reply": "2021-03-01T05:13:14.705774Z"
    },
    "papermill": {
     "duration": 0.037978,
     "end_time": "2021-03-01T05:13:14.705941",
     "exception": false,
     "start_time": "2021-03-01T05:13:14.667963",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Print the Keyword in the 12 topics\n",
    "pprint(lda_model.print_topics())\n",
    "doc_lda = lda_model[corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-01T05:13:14.761244Z",
     "iopub.status.busy": "2021-03-01T05:13:14.760661Z",
     "iopub.status.idle": "2021-03-01T05:13:14.766099Z",
     "shell.execute_reply": "2021-03-01T05:13:14.766940Z"
    },
    "papermill": {
     "duration": 0.040352,
     "end_time": "2021-03-01T05:13:14.767206",
     "exception": false,
     "start_time": "2021-03-01T05:13:14.726854",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Compute Perplexity\n",
    "print('\\nPerplexity: ', lda_model.log_perplexity(corpus))  # a measure of how good the model is. lower the better.\n",
    "\n",
    "# Compute Coherence Score\n",
    "coherence_model_lda = CoherenceModel(model=lda_model, texts=TokenizeReviews, dictionary=id2word, coherence='c_v')\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "print('\\nCoherence Score: ', coherence_lda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.021239,
     "end_time": "2021-03-01T05:13:14.810750",
     "exception": false,
     "start_time": "2021-03-01T05:13:14.789511",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Analyze the topics through the business lens.\n",
    "\n",
    "\n",
    "Here are the possible topic headers\n",
    "\n",
    "  0 - Possible Topic - Lenovo Note K8 (1)\n",
    "  \n",
    "  1 - Possible Topic - First Touch Phone (2)\n",
    "  \n",
    "  2 - Possible Topic - Charging Review (3)\n",
    "  \n",
    "  3 - Possible Topic - Review on sensor time (4)\n",
    "  \n",
    "  4 - Possible Topic - Positive Mobile Review (5) \n",
    "  \n",
    "  5 - Possible Topic - Picture quality (6)\n",
    "  \n",
    "  6 - Possible Topic - Positive Review (5)\n",
    "  \n",
    "  7 - Possible Topic - Review on Processor (7)\n",
    "  \n",
    "  8 - Possible Topic - Positive Review (5)\n",
    "  \n",
    "  9 - Possible Topic - Negative Review (8)\n",
    "  \n",
    "  10 - Possible Topic - Review on Return policy (9)\n",
    "  \n",
    "  11 - Possible Topic - Review on software update (10)\n",
    "  \n",
    "  # Determine which of the topics can be combined.\n",
    "\n",
    "  \n",
    "  Distinct topics can be treated as 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.021132,
     "end_time": "2021-03-01T05:13:14.853530",
     "exception": false,
     "start_time": "2021-03-01T05:13:14.832398",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 10 Create a topic model using LDA with what you think is the optimal number of topics\n",
    "\n",
    "    What is the coherence of the model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-01T05:13:14.903674Z",
     "iopub.status.busy": "2021-03-01T05:13:14.903083Z",
     "iopub.status.idle": "2021-03-01T05:13:14.916989Z",
     "shell.execute_reply": "2021-03-01T05:13:14.917487Z"
    },
    "papermill": {
     "duration": 0.041993,
     "end_time": "2021-03-01T05:13:14.917663",
     "exception": false,
     "start_time": "2021-03-01T05:13:14.875670",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Build LDA model with 8 topics\n",
    "lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
    "                                           id2word=id2word,\n",
    "                                           num_topics=10, \n",
    "                                           random_state=100,\n",
    "                                           update_every=1,\n",
    "                                           chunksize=100,\n",
    "                                           passes=10,\n",
    "                                           alpha='auto',\n",
    "                                           per_word_topics=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-01T05:13:14.975257Z",
     "iopub.status.busy": "2021-03-01T05:13:14.974682Z",
     "iopub.status.idle": "2021-03-01T05:13:14.979447Z",
     "shell.execute_reply": "2021-03-01T05:13:14.979940Z"
    },
    "papermill": {
     "duration": 0.040272,
     "end_time": "2021-03-01T05:13:14.980101",
     "exception": false,
     "start_time": "2021-03-01T05:13:14.939829",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Print the Keyword in the 8 topics\n",
    "pprint(lda_model.print_topics())\n",
    "doc_lda = lda_model[corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-01T05:13:15.030692Z",
     "iopub.status.busy": "2021-03-01T05:13:15.030090Z",
     "iopub.status.idle": "2021-03-01T05:13:15.044877Z",
     "shell.execute_reply": "2021-03-01T05:13:15.044361Z"
    },
    "papermill": {
     "duration": 0.042145,
     "end_time": "2021-03-01T05:13:15.045060",
     "exception": false,
     "start_time": "2021-03-01T05:13:15.002915",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Compute Perplexity\n",
    "print('\\nPerplexity: ', lda_model.log_perplexity(corpus))  # a measure of how good the model is. lower the better.\n",
    "\n",
    "# Compute Coherence Score\n",
    "coherence_model_lda = CoherenceModel(model=lda_model, texts=TokenizeReviews, dictionary=id2word, coherence='c_v')\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "print('\\nCoherence Score: ', coherence_lda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.02333,
     "end_time": "2021-03-01T05:13:15.094577",
     "exception": false,
     "start_time": "2021-03-01T05:13:15.071247",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# The business should be able to interpret the topics.\n",
    "\n",
    "    Name each of the identified topics.\n",
    "\n",
    "    Create a table with the topic name and the top 10 terms in each to present to the business.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.02363,
     "end_time": "2021-03-01T05:13:15.141695",
     "exception": false,
     "start_time": "2021-03-01T05:13:15.118065",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Here are possible topics and and top words for each topic \n",
    "\n",
    "(Topic 1: General Review, \n",
    "\n",
    "  Words: \"heat\" , \"product\" , \"update\" , \"days\" , 1\" , \"play\" , \"software\" , \"need\" , \"user\" , \"ok\"\n",
    "  ),\n",
    "  \n",
    " (Topic 2: Review on Lenovo Note K8,\n",
    " \n",
    "  Words: \"lenovo\" , \"note\" , \"k8\" , \"first\" , \"u\" , \"previous\" , \"mobiles\" , \"still\" , \"face\" , \"office\"\n",
    "  ),\n",
    "  \n",
    " (\n",
    "  Topic 3: Review on Charging time ,\n",
    "  \n",
    "  Words: \"work\" , \"use\" , \"charge\" , \"get\" , \"take\" , \"4\" , \"2\" , \"5\" , \"like\" , \"charger\"\n",
    "  ),\n",
    "  \n",
    " (\n",
    "  Topic 4: Review on Sensor time,\n",
    "  \n",
    "  Words: \"time\" , \"bite\" , \"sensor\" , \"back\" , \"android\" , \"image\" , \"dedicate\" , \"stock\" , \"lot\" , \"music\" \n",
    "  ),\n",
    "  \n",
    " (\n",
    "  Topic 5: Negative Review,\n",
    "  \n",
    "  Words: \"phone\" , \"buy\" , \"dont\" , \"better\" , \"get\" , \"compare\" , \"one\" , \"worst\" , \"last\" , \"service\"\n",
    "  ),\n",
    "  \n",
    " (\n",
    "  Topic 6: Review on redmi ,\n",
    "  \n",
    "  Words: \"poor\" , \"dual\" , \"much\" , \"make\" , \"life\" , \"8\" , \"purchase\" , \"provide\" , \"redmi\" , \"two\"\n",
    "  ),\n",
    "  \n",
    " (\n",
    "  Topic 7: Review on camera,\n",
    "  \n",
    "  Words: \"good\" , \"camera\" , \"quality\" , \"issue\" , \"game\" , \"also\" , \"clarity\" , \"average\" , \"screen\" , \"light\"\n",
    "  ),\n",
    "  \n",
    " (\n",
    "  Topic 8: Review on network,\n",
    "  \n",
    "  Words: \"doesnt\" , \"call\" , \"even\" , \"bad\" , \"network\" , \"many\" , \"cant\" , \"support\" , \"full\" , \"find\"\n",
    "  ),\n",
    " \n",
    " (\n",
    "  Topic 9: Review on battery life,\n",
    "  \n",
    "  Words: \"battery\" , \"feature\" , \"mode\" , \"fast\" , \"drain\" , \"great\" , \"speed\" , \"nice\" , \"device\" , \"really\"\n",
    "  ),\n",
    "  \n",
    " (\n",
    "   Topic 10: Review on price, \n",
    "   \n",
    "   Words: \"mobile\" , \"amazon\" , \"problem\" , \"price\" , \"awesome\" , \"hai\" , \"return\" , \"properly\" , \"best\" , \"hang\"\n",
    "  )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# By:Abdullah Alwabel"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 9.250808,
   "end_time": "2021-03-01T05:13:15.920940",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2021-03-01T05:13:06.670132",
   "version": "2.2.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
